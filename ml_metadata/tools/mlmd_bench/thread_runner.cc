/* Copyright 2020 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "ml_metadata/tools/mlmd_bench/thread_runner.h"

#include <vector>

#include "ml_metadata/metadata_store/metadata_store.h"
#include "ml_metadata/metadata_store/metadata_store_factory.h"
#include "ml_metadata/metadata_store/types.h"
#include "ml_metadata/proto/metadata_store.pb.h"
#include "ml_metadata/tools/mlmd_bench/benchmark.h"
#include "ml_metadata/tools/mlmd_bench/proto/mlmd_bench.pb.h"
#include "ml_metadata/tools/mlmd_bench/stats.h"
#include "ml_metadata/tools/mlmd_bench/workload.h"
#include "tensorflow/core/lib/core/errors.h"
#include "tensorflow/core/lib/core/status.h"
#include "tensorflow/core/lib/core/threadpool.h"

namespace ml_metadata {

ThreadRunner::ThreadRunner(const ConnectionConfig& mlmd_config,
                           const int64 num_threads)
    : mlmd_config_(mlmd_config), num_threads_(num_threads) {}

// The thread runner will first loops over all the executable workloads in
// benchmark and executes them one by one. Each workload will have a
// `thread_stats_list` to record the stats of each thread when executing the
// current workload.
// During the execution, each operation will has a `op_stats` to record current
// operation statistic. Each `op_stats` will be used to update the
// `thread_stats`.
// After the each thread has finished the execution, the workload stats will be
// generated by merging all the thread stats inside the `thread_stats_list`. The
// performance of the workload will be reported according to the workload stats.
tensorflow::Status ThreadRunner::Run(Benchmark& benchmark) {
  for (int i = 0; i < benchmark.num_workloads(); ++i) {
    WorkloadBase* workload = benchmark.workload(i);
    ThreadStats thread_stats_list[num_threads_];
    std::unique_ptr<MetadataStore> set_up_store;
    TF_RETURN_IF_ERROR(CreateMetadataStore(mlmd_config_, &set_up_store));
    TF_RETURN_IF_ERROR(workload->SetUp(set_up_store.get()));
    const int64 op_per_thread = workload->num_operations() / num_threads_;
    {
      // Create a thread pool for multi-thread execution.
      tensorflow::thread::ThreadPool pool(tensorflow::Env::Default(),
                                          "mlmd_bench", num_threads_);
      // `approx_total_done` is used for reporting progress along the way.
      int64 approx_total_done = 0;
      for (int64 thread_index = 0; thread_index < num_threads_;
           ++thread_index) {
        int64 work_items_start_index = op_per_thread * thread_index;
        ThreadStats& curr_thread_stats = thread_stats_list[thread_index];
        pool.Schedule([this, op_per_thread, workload, work_items_start_index,
                       &curr_thread_stats, &approx_total_done]() {
          // Each thread uses a different MLMD client instance to talk to
          // the same back-end.
          std::unique_ptr<MetadataStore> store;
          tensorflow::Status store_status =
              CreateMetadataStore(mlmd_config_, &store);
          // Handles abort issues for concurrent creating `store` of the db.
          while (!store_status.ok()) {
            store_status = CreateMetadataStore(mlmd_config_, &store);
            // If the error is not Abort error, break the current process.
            if (!store_status.ok() &&
                store_status.code() != tensorflow::error::ABORTED) {
              TF_CHECK_OK(store_status);
            }
          }
          curr_thread_stats.Start();
          // Executes the current workload by the specified
          // `work_items_index`.
          int64 work_items_index = work_items_start_index;
          while (work_items_index < work_items_start_index + op_per_thread) {
            // Each operation has a op_stats.
            OpStats op_stats;
            tensorflow::Status status =
                workload->RunOp(work_items_index, store.get(), op_stats);
            // If the error is not Abort error, break the current process.
            if (!status.ok() && status.code() != tensorflow::error::ABORTED) {
              TF_CHECK_OK(status);
            }
            // Handles abort issues for concurrent writing to the db.
            if (!status.ok()) {
              continue;
            }
            work_items_index++;
            approx_total_done++;
            // Updates the current thread stats using the `op_stats`.
            curr_thread_stats.Update(op_stats, approx_total_done);
          }
          curr_thread_stats.Stop();
        });
      }
    }
    TF_RETURN_IF_ERROR(workload->TearDown());
    // Merges all the thread stats of the current workload.
    for (int64 i = 1; i < num_threads_; ++i) {
      thread_stats_list[0].Merge(thread_stats_list[i]);
    }
    // Reports the metrics of interests.
    // TODO(briansong) Return the report as a summary proto and write it to a
    // file.
    thread_stats_list[0].Report(workload->GetName());
  }
  return tensorflow::Status::OK();
}

}  // namespace ml_metadata
